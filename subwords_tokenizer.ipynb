{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "subwords_tokenizer",
      "provenance": [],
      "collapsed_sections": [
        "PxOYBzVqE7mX",
        "8e--LsVQqOI7",
        "Zly47bRHqXiK",
        "l7jW_PuprCuu",
        "NFSRsEw3s4I7"
      ],
      "authorship_tag": "ABX9TyO8EzVRXZDctkMahPNYmMzH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsmttoGtjoMz"
      },
      "source": [
        "Copyright 2019 The TensorFlow Authors.\n",
        "\n",
        "```\n",
        "@title Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS4GXEHwjzZp"
      },
      "source": [
        "# Token ho√° t·ª´ con\n",
        "\n",
        "H∆∞·ªõng d·∫´n n√†y tr√¨nh b√†y l√†m th·∫ø n√†o ƒë·ªÉ t·∫°o ra m·ªôt t·ª´ v·ª±ng t·ª´ con t·ª´ m·ªôt t·∫≠p d·ªØ li·ªáu, v√† s·ª≠ d·ª•ng n√≥ ƒë·ªÉ x√¢y d·ª±ng m·ªôt `text.BertTokenizer` t·ª´ t·ª´ v·ª±ng.\n",
        "\n",
        "∆Øu ƒëi·ªÉm ch√≠nh c·ªßa tokenizer t·ª´ con l√† n√≥ n·ªôi suy gi·ªØa token h√≥a d·ª±a tr√™n t·ª´ v√† tr√™n k√Ω t·ª±. C√°c t·ª´ th√¥ng d·ª•ng c√≥ m·ªôt v·ªã tr√≠ trong t·ª´ v·ª±ng, nh∆∞ng tokenizer c√≥ th·ªÉ r∆°i tr·ªü l·∫°i c√°c m·∫£nh t·ª´ v√† c√°c k√Ω t·ª± ri√™ng ƒë·ªëi v·ªõi c√°c t·ª´ kh√¥ng x√°c ƒë·ªãnh.\n",
        "\n",
        "M·ª•c ti√™u: V√†o cu·ªëi h∆∞·ªõng d·∫´n n√†y b·∫°n s·∫Ω x√¢y d·ª±ng ƒë∆∞·ª£c ho√†n to√†n tokenizer t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi m·∫£nh t·ª´ v√† detokenizer t·ª´ t·∫°p nham, v√† l∆∞u n√≥ nh∆∞ l√† m·ªôt `saved_model` m√† b·∫°n c√≥ th·ªÉ t·∫£i v√† s·ª≠ d·ª•ng trong n√†y [h∆∞·ªõng d·∫´n d·ªãch](https://tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxOYBzVqE7mX"
      },
      "source": [
        "## T·ªïng quan\n",
        "\n",
        "C√°c g√≥i `tensorflow_text` bao g·ªìm tri·ªÉn khai TensorFlow c·ªßa nhi·ªÅu tokenizer chung. ƒêi·ªÅu n√†y bao g·ªìm ba lo·∫°i tokenizers ki·ªÉu t·ª´ con:\n",
        "\n",
        "* `text.BertTokenizer` - L·ªõp BertTokenizer l√† m·ªôt giao di·ªán c·∫•p cao h∆°n. N√≥ bao g·ªìm c√°c thu·∫≠t to√°n t√°ch token c·ªßa Bert v√† m·ªôt `WordPieceTokenizer`. N√≥ l·∫•y **c√°c c√¢u** nh∆∞ ƒë·∫ßu v√†o v√† tr·∫£ v·ªÅ c√°c **token-ID**.\n",
        "\n",
        "* `text.WordpieceTokenizer` - L·ªõp `WordPieceTokenizer` l√† m·ªôt giao di·ªán c·∫•p th·∫•p h∆°n. N√≥ ch·ªâ th·ª±c hi·ªán [thu·∫≠t to√°n WordPiece](https://render.githubusercontent.com/view/ipynb?color_mode=auto&commit=76788f6fd58fb2e4c3d7de3e95ea70f0a671ad97&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f74656e736f72666c6f772f746578742f373637383866366664353866623265346333643764653365393565613730663061363731616439372f646f63732f67756964652f737562776f7264735f746f6b656e697a65722e6970796e62&nwo=tensorflow%2Ftext&path=docs%2Fguide%2Fsubwords_tokenizer.ipynb&repository_id=189305903&repository_type=Repository#applying_wordpiece). B·∫°n ph·∫£i chu·∫©n h√≥a v√† t√°ch vƒÉn b·∫£n th√†nh c√°c t·ª´ tr∆∞·ªõc khi g·ªçi n√≥. N√≥ l·∫•y **c√°c t·ª´** nh∆∞ ƒë·∫ßu v√†o v√† tr·∫£ v·ªÅ c√°c **token-ID**.\n",
        "\n",
        "* `text.SentencepieceTokenizer` - `SentencepieceTokenizer` y√™u c·∫ßu m·ªôt thi·∫øt l·∫≠p ph·ª©c t·∫°p h∆°n. B·ªô kh·ªüi t·∫°o c·ªßa n√≥ y√™u c·∫ßu m·ªôt m√¥ h√¨nh m·∫£nh c√¢u ti·ªÅn ƒë√†o t·∫°o. Xem [kho google/sentencepiece](https://github.com/google/sentencepiece#train-sentencepiece-model) ƒë·ªÉ ƒë∆∞·ª£c h∆∞·ªõng d·∫´n l√†m th·∫ø n√†o x√¢y d·ª±ng m·ªôt trong nh·ªØng m√¥ h√¨nh n√†y. N√≥ c√≥ th·ªÉ ch·∫•p nh·∫≠n **c√°c c√¢u** nh∆∞ ƒë·∫ßu v√†o khi token ho√°.\n",
        "\n",
        "H∆∞·ªõng d·∫´n n√†y x√¢y d·ª±ng m·ªôt t·ª´ v·ª±ng M·∫£nh t·ª´ theo c√°ch t·ª´ tr√™n xu·ªëng, b·∫Øt ƒë·∫ßu t·ª´ c√°c t·ª´ hi·ªán c√≥. Qu√° tr√¨nh n√†y kh√¥ng ho·∫°t ƒë·ªông ·ªü ti·∫øng Nh·∫≠t, ti·∫øng Trung ho·∫∑c ti·∫øng H√†n v√¨ nh·ªØng ng√¥n ng·ªØ n√†y kh√¥ng c√≥ c√°c ƒë∆°n v·ªã nhi·ªÅu k√Ω t·ª± r√µ r√†ng. ƒê·ªÉ token ho√° c√°c ng√¥n ng·ªØ ƒë√≥ c√¢n nh·∫Øc s·ª≠ d·ª•ng `text.SentencepieceTokenizer`, `text.UnicodeCharTokenizer` ho·∫∑c [c√°ch ti·∫øp c·∫≠n n√†y](https://tfhub.dev/google/zh_segmentation/1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e--LsVQqOI7"
      },
      "source": [
        "## C√†i ƒë·∫∑t\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrcCCuMhqQ9E"
      },
      "source": [
        "!pip install -q -U tensorflow-text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb0G1l_oqRnr"
      },
      "source": [
        "!pip install -q tensorflow_datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7I3jsrbqS5x"
      },
      "source": [
        "import collections\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import tempfile\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caetoEsgqU9J"
      },
      "source": [
        "tf.get_logger().setLevel('ERROR')\n",
        "pwd = pathlib.Path.cwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zly47bRHqXiK"
      },
      "source": [
        "## T·∫£i v·ªÅ t·∫≠p d·ªØ li·ªáu\n",
        "\n",
        "L·∫•y t·∫≠p d·ªØ li·ªáu d·ªãch ti·∫øng B·ªì ƒê√†o Nha/Anh t·ª´ [tfds](https://tensorflow.org/datasets):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b3xv1_-qjCZ"
      },
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
        "                               as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seQT2q2sqlS3"
      },
      "source": [
        "T·∫≠p d·ªØ li·ªáu n√†y t·∫°o ra c√°c c·∫∑p c√¢u ti·∫øng B·ªì ƒê√†o Nha/Anh:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naQZe5HbqyDV"
      },
      "source": [
        "for pt, en in train_examples.take(1):\n",
        "  print(\"Portuguese: \", pt.numpy().decode('utf-8'))\n",
        "  print(\"English:   \", en.numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0DaiHgPq2kE"
      },
      "source": [
        "L∆∞u √Ω m·ªôt s·ªë ƒëi·ªÅu v·ªÅ c√°c c√¢u v√≠ d·ª• ·ªü tr√™n:\n",
        "\n",
        "* Ch√∫ng l√† ch·ªØ th∆∞·ªùng.\n",
        "* C√≥ kho·∫£ng tr·∫Øng xung quanh d·∫•u c√¢u.\n",
        "* Kh√¥ng r√µ li·ªáu chu·∫©n h√≥a unicode c√≥ ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng hay kh√¥ng."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huSfCz59rAHo"
      },
      "source": [
        "train_en = train_examples.map(lambda pt, en: en)\n",
        "train_pt = train_examples.map(lambda pt, en: pt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7jW_PuprCuu"
      },
      "source": [
        "## T·∫°o t·ª´ v·ª±ng\n",
        "\n",
        "Ph·∫ßn n√†y t·∫°o ra m·ªôt t·ª´ v·ª±ng m·∫£nh t·ª´ t·ª´ m·ªôt t·∫≠p d·ªØ li·ªáu. N·∫øu b·∫°n ƒë√£ c√≥ m·ªôt t·∫≠p tin t·ª´ v·ª±ng v√† ch·ªâ mu·ªën xem l√†m th·∫ø n√†o ƒë·ªÉ x√¢y d·ª±ng m·ªôt `text.BertTokenizer` ho·∫∑c tolenizer `text.Wordpiece` v·ªõi n√≥ th√¨ b·∫°n c√≥ th·ªÉ b·ªè qua th·∫≥ng ƒë·∫øn ph·∫ßn [x√¢y d·ª±ng c√°c tokenizer](https://render.githubusercontent.com/view/ipynb?color_mode=auto&commit=76788f6fd58fb2e4c3d7de3e95ea70f0a671ad97&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f74656e736f72666c6f772f746578742f373637383866366664353866623265346333643764653365393565613730663061363731616439372f646f63732f67756964652f737562776f7264735f746f6b656e697a65722e6970796e62&nwo=tensorflow%2Ftext&path=docs%2Fguide%2Fsubwords_tokenizer.ipynb&repository_id=189305903&repository_type=Repository#build_the_tokenizer).\n",
        "\n",
        "L∆∞u √Ω: M√£ sinh t·ª´ v·ª±ng ƒë∆∞·ª£c s·ª≠ d·ª•ng trong h∆∞·ªõng d·∫´n n√†y ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a cho **ƒë∆°n gi·∫£n**. N·∫øu b·∫°n c·∫ßn m·ªôt gi·∫£i ph√°p m·ªü r·ªông h∆°n xem x√©t s·ª≠ d·ª•ng tri·ªÉn khai Apache Beam s·∫µn c√≥ trong [tools/wordpiece_vocab/generate_vocab.py](https://github.com/tensorflow/text/blob/master/tensorflow_text/tools/wordpiece_vocab/generate_vocab.py)\n",
        "\n",
        "\n",
        "M√£ sinh t·ª´ v·ª±ng ƒë∆∞·ª£c bao g·ªìm trong g√≥i pip `tensorflow_text`. N√≥ kh√¥ng ƒë∆∞·ª£c nh·∫≠p m·∫∑c ƒë·ªãnh n√™n b·∫°n c·∫ßn ph·∫£i nh·∫≠p th·ªß c√¥ng:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LqwHy2jrCY-"
      },
      "source": [
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TisYjtzur5P3"
      },
      "source": [
        "H√†m `bert_vocab.bert_vocab_from_dataset` s·∫Ω t·∫°o ra c√°c t·ª´ v·ª±ng.\n",
        "\n",
        "C√≥ nhi·ªÅu ƒë·ªëi s·ªë b·∫°n c√≥ th·ªÉ ƒë·∫∑t ƒë·ªÉ ƒëi·ªÅu ch·ªânh h√†nh vi c·ªßa n√≥. ƒê·ªëi v·ªõi h∆∞·ªõng d·∫´n n√†y, b·∫°n s·∫Ω ch·ªß y·∫øu s·ª≠ d·ª•ng c√°c gi√° tr·ªã m·∫∑c ƒë·ªãnh. N·∫øu b·∫°n mu·ªën t√¨m hi·ªÉu th√™m v·ªÅ c√°c t√πy ch·ªçn, ƒë·∫ßu ti√™n ƒë·ªçc [thu·∫≠t to√°n](https://render.githubusercontent.com/view/ipynb?color_mode=auto&commit=76788f6fd58fb2e4c3d7de3e95ea70f0a671ad97&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f74656e736f72666c6f772f746578742f373637383866366664353866623265346333643764653365393565613730663061363731616439372f646f63732f67756964652f737562776f7264735f746f6b656e697a65722e6970796e62&nwo=tensorflow%2Ftext&path=docs%2Fguide%2Fsubwords_tokenizer.ipynb&repository_id=189305903&repository_type=Repository#algorithm), v√† sau ƒë√≥ c√≥ nh√¨n v√†o [m√£ n√†y](https://github.com/tensorflow/text/blob/master/tensorflow_text/tools/wordpiece_vocab/bert_vocab_from_dataset.py).\n",
        "\n",
        "Qu√° tr√¨nh n√†y m·∫•t kho·∫£ng 2 ph√∫t."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16kSnNjKsRSu"
      },
      "source": [
        "bert_tokenizer_params=dict(lower_case=True)\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "bert_vocab_args = dict(\n",
        "    # The target vocabulary size\n",
        "    vocab_size = 8000,\n",
        "    # Reserved tokens that must be included in the vocabulary\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    # Arguments for `text.BertTokenizer`\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\n",
        "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "    learn_params={},\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuL3o6zKsTfR"
      },
      "source": [
        "%%time\n",
        "pt_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_pt.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0TMfvvJsU92"
      },
      "source": [
        "ƒê√¢y l√† c√°c v·∫øt c·∫Øt c·ªßa k·∫øt qu·∫£ t·ª´ v·ª±ng."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAQ2O4bFsh1x"
      },
      "source": [
        "print(pt_vocab[:10])\n",
        "print(pt_vocab[100:110])\n",
        "print(pt_vocab[1000:1010])\n",
        "print(pt_vocab[-10:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vePXlMtJskR4"
      },
      "source": [
        "Vi·∫øt t·∫≠p tin t·ª´ v·ª±ng:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9rxQJ4asme7"
      },
      "source": [
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QomCN--Wsp7Z"
      },
      "source": [
        "write_vocab_file('pt_vocab.txt', pt_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wvu8Satsrqf"
      },
      "source": [
        "S·ª≠ d·ª•ng h√†m ƒë√≥ ƒë·ªÉ t·∫°o t·ª´ v·ª±ng t·ª´ d·ªØ li·ªáu ti·∫øng Anh:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5P05WvUsu3C"
      },
      "source": [
        "%%time\n",
        "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_en.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_3WNyetswmC"
      },
      "source": [
        "print(en_vocab[:10])\n",
        "print(en_vocab[100:110])\n",
        "print(en_vocab[1000:1010])\n",
        "print(en_vocab[-10:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg4J0qf2sx1m"
      },
      "source": [
        "ƒê√¢y l√† hai t·∫≠p tin t·ª´ v·ª±ng:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14HvaAkfs1GD"
      },
      "source": [
        "write_vocab_file('en_vocab.txt', en_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhSsa4Aas2RF"
      },
      "source": [
        "!ls *.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFSRsEw3s4I7"
      },
      "source": [
        "## X√¢y d·ª±ng tokenizer\n",
        "\n",
        "`text.BertTokenizer` c√≥ th·ªÉ ƒë∆∞·ª£c kh·ªüi t·∫°o b·∫±ng c√°ch truy·ªÅn ƒë∆∞·ªùng d·∫´n c·ªßa t·∫≠p tin t·ª´ v·ª±ng l√†m ƒë·ªëi s·ªë ƒë·∫ßu ti√™n (xem ph·∫ßn [tf.lookup](https://render.githubusercontent.com/view/ipynb?color_mode=auto&commit=76788f6fd58fb2e4c3d7de3e95ea70f0a671ad97&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f74656e736f72666c6f772f746578742f373637383866366664353866623265346333643764653365393565613730663061363731616439372f646f63732f67756964652f737562776f7264735f746f6b656e697a65722e6970796e62&nwo=tensorflow%2Ftext&path=docs%2Fguide%2Fsubwords_tokenizer.ipynb&repository_id=189305903&repository_type=Repository#tf.lookup) ƒë·ªÉ bi·∫øt c√°c t√πy ch·ªçn kh√°c):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IXwU2QttIOe"
      },
      "source": [
        "pt_tokenizer = text.BertTokenizer('pt_vocab.txt', **bert_tokenizer_params)\n",
        "en_tokenizer = text.BertTokenizer('en_vocab.txt', **bert_tokenizer_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2CU5eRatKhX"
      },
      "source": [
        "B√¢y gi·ªù b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng n√≥ ƒë·ªÉ m√£ h√≥a m·ªôt s·ªë vƒÉn b·∫£n. L·∫•y m·ªôt l√¥ 3 v√≠ d·ª• t·ª´ d·ªØ li·ªáu ti·∫øng Anh:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4mGFL2CtNfM"
      },
      "source": [
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  for ex in en_examples:\n",
        "    print(ex.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8Gc0ZgytP_a"
      },
      "source": [
        "Ch·∫°y n√≥ th√¥ng qua ph∆∞∆°ng th·ª©c `BertTokenizer.tokenize`. Ban ƒë·∫ßu, n√≥ tr·∫£ v·ªÅ m·ªôt `tf.RaggedTensor` v·ªõi c√°c tr·ª•c `(batch, word, word-piece)`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnA4CtxUtZkX"
      },
      "source": [
        "# Tokenize the examples -> (batch, word, word-piece)\n",
        "token_batch = en_tokenizer.tokenize(en_examples)\n",
        "# Merge the word and word-piece axes -> (batch, tokens)\n",
        "token_batch = token_batch.merge_dims(-2,-1)\n",
        "\n",
        "for ex in token_batch.to_list():\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmrhF3r1tbLx"
      },
      "source": [
        "N·∫øu b·∫°n thay th·∫ø c√°c ID token b·∫±ng c√°c bi·ªÉu di·ªÖn vƒÉn b·∫£n c·ªßa ch√∫ng (s·ª≠ d·ª•ng `tf.gather`), b·∫°n c√≥ th·ªÉ th·∫•y r·∫±ng trong v√≠ d·ª• ƒë·∫ßu ti√™n, c√°c t·ª´ \"`searchability`\" v√† \"`serendipity`\" ƒë√£ ƒë∆∞·ª£c ph√¢n t√°ch th√†nh \"`search ##ability`\" v√† \"`s ##ere ##nd ##ip ##ity`\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtyHqMLytwtP"
      },
      "source": [
        "# Lookup each token id in the vocabulary.\n",
        "txt_tokens = tf.gather(en_vocab, token_batch)\n",
        "# Join with spaces.\n",
        "tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZBCzayJtzZr"
      },
      "source": [
        "ƒê·ªÉ t·∫≠p h·ª£p l·∫°i c√°c t·ª´ t·ª´ c√°c token ƒë∆∞·ª£c tr√≠ch xu·∫•t, s·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c `BertTokenizer.detokenize`:\n",
        "\n",
        "> L∆∞u √Ω: `BertTokenizer.tokenize`/`BertTokenizer.detokenize` kh√¥ng kh·ª© h·ªìi losslessly. K·∫øt qu·∫£ c·ªßa `detokenize` s·∫Ω kh√¥ng, n√≥i chung, c√≥ c√πng m·ªôt n·ªôi dung ho·∫∑c c√°c b√π ƒë·∫Øp nh∆∞ ƒë·∫ßu v√†o ƒë·ªÉ `tokenize`. ƒêi·ªÅu n√†y l√† do b∆∞·ªõc \"token ho√° c∆° b·∫£n\", ƒë·ªÉ t√°ch chu·ªói th√†nh c√°c t·ª´ tr∆∞·ªõc khi √°p d·ª•ng `WordpieceTokenizer`, bao g·ªìm c√°c b∆∞·ªõc kh√¥ng th·ªÉ ƒë·∫£o ng∆∞·ª£c nh∆∞ ch·ªØ th∆∞·ªùng v√† t√°ch theo d·∫•u c√¢u. `WordpieceTokenizer` ·ªü m·∫∑t kh√°c **c√≥ th·ªÉ ƒë·∫£o ng∆∞·ª£c**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8u0ev-Lu3jj"
      },
      "source": [
        "## T√πy ch·ªânh v√† xu·∫•t\n",
        "\n",
        "H∆∞·ªõng d·∫´n n√†y ƒë∆∞·ª£c x√¢y d·ª±ng tokenizer vƒÉn b·∫£n v√† detokenizer s·ª≠ d·ª•ng b·ªüi c√°c h∆∞·ªõng d·∫´n [Transformer](https://tensorflow.org/text/tutorials/transformer). Ph·∫ßn n√†y th√™m c√°c ph∆∞∆°ng th·ª©c v√† c√°c b∆∞·ªõc x·ª≠ l√Ω ƒë·ªÉ ƒë∆°n gi·∫£n h√≥a h∆∞·ªõng d·∫´n, v√† xu·∫•t c√°c tokenizer s·ª≠ d·ª•ng `tf.saved_model` ƒë·ªÉ h·ªç c√≥ th·ªÉ ƒë∆∞·ª£c nh·∫≠p b·ªüi c√°c h∆∞·ªõng d·∫´n kh√°c."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDZtHONrvQFS"
      },
      "source": [
        "### Tu·ª≥ ch·ªânh token ho√°\n",
        "\n",
        "C√°c h∆∞·ªõng d·∫´n d∆∞·ªõi ƒë√¢y mong ƒë·ª£i vƒÉn b·∫£n ƒë√£ token ho√° bao g·ªìm c√°c token `[START]` v√† `[END]`.\n",
        "\n",
        "C√°c `reserved_tokens` gi·ªØ l·∫°i kh√¥ng gian ·ªü ph·∫ßn ƒë·∫ßu c·ªßa t·ª´ v·ª±ng, v√¨ v·∫≠y `[START]` v√† `[END]` c√≥ c√πng c√°c ch·ªâ s·ªë ho c·∫£ hai ng√¥n ng·ªØ:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqU_d8Pov5Q4"
      },
      "source": [
        "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
        "\n",
        "def add_start_end(ragged):\n",
        "  count = ragged.bounding_shape()[0]\n",
        "  starts = tf.fill([count,1], START)\n",
        "  ends = tf.fill([count,1], END)\n",
        "  return tf.concat([starts, ragged, ends], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBNOTaQBv6F1"
      },
      "source": [
        "words = en_tokenizer.detokenize(add_start_end(token_batch))\n",
        "tf.strings.reduce_join(words, separator=' ', axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJn-yVpZv77f"
      },
      "source": [
        "### Tu·ª≥ ch·ªânh detoken ho√°\n",
        "\n",
        "Tr∆∞·ªõc khi xu·∫•t c√°c `tokenizer`, c√≥ m·ªôt s·ªë th·ª© b·∫°n c√≥ th·ªÉ d·ªçn d·∫πp cho c√°c h∆∞·ªõng d·∫´n ph√≠a d∆∞·ªõi:\n",
        "\n",
        "1. H·ªç mu·ªën t·∫°o ƒë·∫ßu ra vƒÉn b·∫£n s·∫°ch, v√¨ v·∫≠y th·∫£ tokens ƒë·ªÉ d√†nh nh∆∞ `[START]`, `[END]` v√† `[PAD]`.\n",
        "\n",
        "2. H·ªç ƒëang quan t√¢m trong c√°c chu·ªói ho√†n ch·ªânh, v√¨ v·∫≠y √°p d·ª•ng m·ªôt chu·ªói n·ªëi d·ªçc theo tr·ª•c `words` c·ªßa k·∫øt qu·∫£."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwCVzfkFweQp"
      },
      "source": [
        "def cleanup_text(reserved_tokens, token_txt):\n",
        "  # Drop the reserved tokens, except for \"[UNK]\".\n",
        "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "  bad_token_re = \"|\".join(bad_tokens)\n",
        "    \n",
        "  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
        "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "\n",
        "  # Join them into strings.\n",
        "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZyOPBQQwfkd"
      },
      "source": [
        "en_examples.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO9qsa1Fwg8J"
      },
      "source": [
        "token_batch = en_tokenizer.tokenize(en_examples).merge_dims(-2,-1)\n",
        "words = en_tokenizer.detokenize(token_batch)\n",
        "words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko20Z9g-wib0"
      },
      "source": [
        "cleanup_text(reserved_tokens, words).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhubd2cOwky1"
      },
      "source": [
        "### Xu·∫•t\n",
        "\n",
        "Kh·ªëi m√£ sau x√¢y d·ª±ng m·ªôt l·ªõp `CustomTokenizer` ƒë·ªÉ ch·ª©a c√°c th·ªÉ hi·ªán `text.BertTokenizer`, logic t√πy ch·ªânh, v√† l·ªõp b·ªçc `@tf.function` c·∫ßn thi·∫øt cho xu·∫•t."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIyKM5kCw999"
      },
      "source": [
        "class CustomTokenizer(tf.Module):\n",
        "  def __init__(self, reserved_tokens, vocab_path):\n",
        "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
        "    self._reserved_tokens = reserved_tokens\n",
        "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
        "\n",
        "    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
        "    self.vocab = tf.Variable(vocab)\n",
        "\n",
        "    ## Create the signatures for export:   \n",
        "\n",
        "    # Include a tokenize signature for a batch of strings. \n",
        "    self.tokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
        "    \n",
        "    # Include `detokenize` and `lookup` signatures for:\n",
        "    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
        "    #   * `RaggedTensors` with shape [batch, tokens]\n",
        "    self.detokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.detokenize.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    self.lookup.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.lookup.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    # These `get_*` methods take no arguments\n",
        "    self.get_vocab_size.get_concrete_function()\n",
        "    self.get_vocab_path.get_concrete_function()\n",
        "    self.get_reserved_tokens.get_concrete_function()\n",
        "    \n",
        "  @tf.function\n",
        "  def tokenize(self, strings):\n",
        "    enc = self.tokenizer.tokenize(strings)\n",
        "    # Merge the `word` and `word-piece` axes.\n",
        "    enc = enc.merge_dims(-2,-1)\n",
        "    enc = add_start_end(enc)\n",
        "    return enc\n",
        "\n",
        "  @tf.function\n",
        "  def detokenize(self, tokenized):\n",
        "    words = self.tokenizer.detokenize(tokenized)\n",
        "    return cleanup_text(self._reserved_tokens, words)\n",
        "\n",
        "  @tf.function\n",
        "  def lookup(self, token_ids):\n",
        "    return tf.gather(self.vocab, token_ids)\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_size(self):\n",
        "    return tf.shape(self.vocab)[0]\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_path(self):\n",
        "    return self._vocab_path\n",
        "\n",
        "  @tf.function\n",
        "  def get_reserved_tokens(self):\n",
        "    return tf.constant(self._reserved_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7oBH5R2w_PU"
      },
      "source": [
        "X√¢y d·ª±ng m·ªôt `CustomTokenizer` cho m·ªói ng√¥n ng·ªØ:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On9nVyCOxBdL"
      },
      "source": [
        "tokenizers = tf.Module()\n",
        "tokenizers.pt = CustomTokenizer(reserved_tokens, 'pt_vocab.txt')\n",
        "tokenizers.en = CustomTokenizer(reserved_tokens, 'en_vocab.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtklW4lfxE_N"
      },
      "source": [
        "Xu·∫•t c√°c tokenizer d∆∞·ªõi d·∫°ng `save_model`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkSSucA0xIEG"
      },
      "source": [
        "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
        "tf.saved_model.save(tokenizers, model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQv0QQwOxJTr"
      },
      "source": [
        "T·∫£i l·∫°i `saved_model` v√† ki·ªÉm tra c√°c ph∆∞∆°ng th·ª©c:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t48M_dI-xOsR"
      },
      "source": [
        "reloaded_tokenizers = tf.saved_model.load(model_name)\n",
        "reloaded_tokenizers.en.get_vocab_size().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVwu0EfXxPoI"
      },
      "source": [
        "tokens = reloaded_tokenizers.en.tokenize(['Hello TensorFlow!'])\n",
        "tokens.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zc9CogixQrC"
      },
      "source": [
        "text_tokens = reloaded_tokenizers.en.lookup(tokens)\n",
        "text_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVnkakVFxR8y"
      },
      "source": [
        "round_trip = reloaded_tokenizers.en.detokenize(tokens)\n",
        "\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8EtW-7cxTvh"
      },
      "source": [
        "L∆∞u tr·ªØ n√≥ cho [c√°c h∆∞·ªõng d·∫´n d·ªãch](https://tensorflow.org/text/tutorials/transformer):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijBl6-UNxW-H"
      },
      "source": [
        "!zip -r {model_name}.zip {model_name}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rWwV0xqxYLy"
      },
      "source": [
        "!du -h *.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gHNy3xpxZta"
      },
      "source": [
        "## Tu·ª≥ ch·ªçn: Thu·∫≠t to√°n\n",
        "\n",
        "ƒêi·ªÅu ƒë√°ng ch√∫ √Ω ·ªü ƒë√¢y l√† c√≥ hai phi√™n b·∫£n c·ªßa thu·∫≠t to√°n WordPiece: D∆∞·ªõi-l√™n v√† tr√™n-xu·ªëng. Trong c·∫£ hai tr∆∞·ªùng h·ª£p, m·ª•c ti√™u l√† nh∆∞ nhau: \"*V·ªõi m·ªôt kho ng·ªØ li·ªáu ƒë√†o t·∫°o v√† m·ªôt s·ªë token D mong mu·ªën, v·∫•n ƒë·ªÅ t·ªëi ∆∞u h√≥a l√† ch·ªçn c√°c m·∫£nh t·ª´ D sao cho kho ng·ªØ li·ªáu thu ƒë∆∞·ª£c l√† t·ªëi thi·ªÉu v·ªÅ s·ªë l∆∞·ª£ng m·∫£nh t·ª´ ch·ªØ khi ƒë∆∞·ª£c ph√¢n ƒëo·∫°n theo m√¥ h√¨nh m·∫£nh t·ª´ ƒë√£ ch·ªçn.*\"\n",
        "\n",
        "B·∫£n g·ªëc [thu·∫≠t to√°n WordPiece t·ª´ d∆∞·ªõi-l√™n](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf), d·ª±a tr√™n [m√£ h√≥a c·∫∑p byte](https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10) . Gi·ªëng nh∆∞ BPE, N√≥ b·∫Øt ƒë·∫ßu v·ªõi b·∫£ng ch·ªØ c√°i v√† k·∫øt h·ª£p l·∫∑p ƒëi l·∫∑p l·∫°i c√°c bigram th√¥ng th∆∞·ªùng ƒë·ªÉ t·∫°o th√†nh c√°c m·∫£nh t·ª´ v√† t·ª´.\n",
        "\n",
        "B·ªô t·∫°o t·ª´ v·ª±ng c·ªßa TensorFlow Text theo sau vi·ªác th·ª±c hi·ªán t·ª´ tr√™n-xu·ªëng t·ª´ [Bert](https://arxiv.org/pdf/1810.04805.pdf). B·∫Øt ƒë·∫ßu v·ªõi c√°c t·ª´ v√† chia nh·ªè ch√∫ng th√†nh c√°c th√†nh ph·∫ßn nh·ªè h∆°n cho ƒë·∫øn khi ch√∫ng ƒë·∫°t ƒë·∫øn ng∆∞·ª°ng t·∫ßn su·∫•t, ho·∫∑c kh√¥ng th·ªÉ chia nh·ªè h∆°n n·ªØa. Ph·∫ßn ti·∫øp theo m√¥ t·∫£ chi ti·∫øt ƒëi·ªÅu n√†y. ƒê·ªëi v·ªõi ti·∫øng Nh·∫≠t, ti·∫øng Trung v√† ti·∫øng H√†n, c√°ch ti·∫øp c·∫≠n t·ª´ tr√™n-xu·ªëng n√†y kh√¥ng ho·∫°t ƒë·ªông v√¨ kh√¥ng c√≥ ƒë∆°n v·ªã t·ª´ r√µ r√†ng n√†o ƒë·ªÉ b·∫Øt ƒë·∫ßu. ƒê·ªëi v·ªõi nh·ªØng b·∫°n c·∫ßn m·ªôt [c√°ch ti·∫øp c·∫≠n kh√°c](https://tfhub.dev/google/zh_segmentation/1).\n",
        "\n",
        "(Ph·∫ßn sau d√†i qu√° n√™n l∆∞·ªùi... üòÅ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp2QtW1gy6-I"
      },
      "source": [
        "### Choosing the vocabulary\n",
        "\n",
        "The top-down WordPiece generation algorithm takes in a set of (word, count) pairs and a threshold `T`, and returns a vocabulary `V`.\n",
        "\n",
        "The algorithm is iterative. It is run for `k` iterations, where typically `k = 4`, but only the first two are really important. The third and fourth (and beyond) are just identical to the second. Note that each step of the binary search runs the algorithm from scratch for `k` iterations.\n",
        "\n",
        "The iterations described below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXFtLt0by8_1"
      },
      "source": [
        "#### First iteration\n",
        "\n",
        "1.  Iterate over every word and count pair in the input, denoted as `(w, c)`.\n",
        "2.  For each word `w`, generate every substring, denoted as `s`. E.g., for the\n",
        "    word `human`, we generate `{h, hu, hum, huma,\n",
        "    human, ##u, ##um, ##uma, ##uman, ##m, ##ma, ##man, #a, ##an, ##n}`.\n",
        "3.  Maintain a substring-to-count hash map, and increment the count of each `s`\n",
        "    by `c`. E.g., if we have `(human, 113)` and `(humas, 3)` in our input, the\n",
        "    count of `s = huma` will be `113+3=116`.\n",
        "4.  Once we've collected the counts of every substring, iterate over the `(s,\n",
        "    c)` pairs *starting with the longest `s` first*.\n",
        "5.  Keep any `s` that has a `c > T`. E.g., if `T = 100` and we have `(pers,\n",
        "    231); (dogs, 259); (##rint; 76)`, then we would keep `pers` and `dogs`.\n",
        "6.  When an `s` is kept, subtract off its count from all of its prefixes. This\n",
        "    is the reason for sorting all of the `s` by length in step 4. This is a\n",
        "    critical part of the algorithm, because otherwise words would be double\n",
        "    counted. For example, let's say that we've kept `human` and we get to\n",
        "    `(huma, 116)`. We know that `113` of those `116` came from `human`, and `3`\n",
        "    came from `humas`. However, now that `human` is in our vocabulary, we know\n",
        "    we will never segment `human` into `huma ##n`. So once `human` has been\n",
        "    kept, then `huma` only has an *effective* count of `3`.\n",
        "\n",
        "This algorithm will generate a set of word pieces `s` (many of which will be\n",
        "whole words `w`), which we *could* use as our WordPiece vocabulary.\n",
        "\n",
        "However, there is a problem: This algorithm will severely overgenerate word\n",
        "pieces. The reason is that we only subtract off counts of prefix tokens.\n",
        "Therefore, if we keep the word `human`, we will subtract off the count for `h,\n",
        "hu, hu, huma`, but not for `##u, ##um, ##uma, ##uman` and so on. So we might\n",
        "generate both `human` and `##uman` as word pieces, even though `##uman` will\n",
        "never be applied.\n",
        "\n",
        "So why not subtract off the counts for every *substring*, not just every\n",
        "*prefix*? Because then we could end up subtracting off the counts multiple\n",
        "times. Let's say that we're processing `s` of length 5 and we keep both\n",
        "`(##denia, 129)` and `(##eniab, 137)`, where `65` of those counts came from the\n",
        "word `undeniable`. If we subtract off from *every* substring, we would subtract\n",
        "`65` from the substring `##enia` twice, even though we should only subtract\n",
        "once. However, if we only subtract off from prefixes, it will correctly only be\n",
        "subtracted once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byOuWJrx0gHs"
      },
      "source": [
        "### Applying WordPiece\n",
        "\n",
        "<a id=\"applying_wordpiece\"></a>\n",
        "\n",
        "Once a WordPiece vocabulary has been generated, we need to be able to apply it\n",
        "to new data. The algorithm is a simple greedy longest-match-first application.\n",
        "\n",
        "For example, consider segmenting the word `undeniable`.\n",
        "\n",
        "We first lookup `undeniable` in our WordPiece dictionary, and if it's present,\n",
        "we're done. If not, we decrement the end point by one character, and repeat,\n",
        "e.g., `undeniabl`.\n",
        "\n",
        "Eventually, we will either find a subtoken in our vocabulary, or get down to a\n",
        "single character subtoken. (In general, we assume that every character is in our\n",
        "vocabulary, although this might not be the case for rare Unicode characters. If\n",
        "we encounter a rare Unicode character that's not in the vocabulary we simply map\n",
        "the entire word to `<unk>`).\n",
        "\n",
        "In this case, we find `un` in our vocabulary. So that's our first word piece.\n",
        "Then we jump to the end of `un` and repeat the processing, e.g., try to find\n",
        "`##deniable`, then `##deniabl`, etc. This is repeated until we've segmented the\n",
        "entire word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHDTYkDi0jvV"
      },
      "source": [
        "## Optional: tf.lookup\n",
        "\n",
        "<a id=\"tf.lookup\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIOEoXpm0kk-"
      },
      "source": [
        "If you need access to, or more control over the vocabulary it's worth noting that you can build the lookup table yourself and pass that to `BertTokenizer`.\n",
        "\n",
        "When you pass a string, `BertTokenizer` does the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05TLvUmD0m_x"
      },
      "source": [
        "pt_lookup = tf.lookup.StaticVocabularyTable(\n",
        "    num_oov_buckets=1,\n",
        "    initializer=tf.lookup.TextFileInitializer(\n",
        "        filename='pt_vocab.txt',\n",
        "        key_dtype=tf.string,\n",
        "        key_index = tf.lookup.TextFileIndex.WHOLE_LINE,\n",
        "        value_dtype = tf.int64,\n",
        "        value_index=tf.lookup.TextFileIndex.LINE_NUMBER)) \n",
        "pt_tokenizer = text.BertTokenizer(pt_lookup)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir-eeDMA0pSH"
      },
      "source": [
        "Now you have direct access to the lookup table used in the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_HNkH_L0rBX"
      },
      "source": [
        "pt_lookup.lookup(tf.constant(['eÃÅ', 'um', 'uma', 'para', 'naÃÉo']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aERcCr60tdI"
      },
      "source": [
        "You don't need to use a vocabulary file, `tf.lookup` has other initializer options. If you have the vocabulary in memory you can use `lookup.KeyValueTensorInitializer`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjJtBYLp0uiM"
      },
      "source": [
        "pt_lookup = tf.lookup.StaticVocabularyTable(\n",
        "    num_oov_buckets=1,\n",
        "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=pt_vocab,\n",
        "        values=tf.range(len(pt_vocab), dtype=tf.int64))) \n",
        "pt_tokenizer = text.BertTokenizer(pt_lookup)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}