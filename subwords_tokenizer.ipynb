{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "subwords_tokenizer",
      "provenance": [],
      "collapsed_sections": [
        "PxOYBzVqE7mX",
        "8e--LsVQqOI7",
        "Zly47bRHqXiK",
        "l7jW_PuprCuu",
        "NFSRsEw3s4I7"
      ],
      "authorship_tag": "ABX9TyO8EzVRXZDctkMahPNYmMzH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2019 The TensorFlow Authors.\n",
        "\n",
        "```\n",
        "@title Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "```"
      ],
      "metadata": {
        "id": "rsmttoGtjoMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token hoá từ con\n",
        "\n",
        "Hướng dẫn này trình bày làm thế nào để tạo ra một từ vựng từ con từ một tập dữ liệu, và sử dụng nó để xây dựng một `text.BertTokenizer` từ từ vựng.\n",
        "\n",
        "Ưu điểm chính của tokenizer từ con là nó nội suy giữa token hóa dựa trên từ và trên ký tự. Các từ thông dụng có một vị trí trong từ vựng, nhưng tokenizer có thể rơi trở lại các mảnh từ và các ký tự riêng đối với các từ không xác định.\n",
        "\n",
        "Mục tiêu: Vào cuối hướng dẫn này bạn sẽ xây dựng được hoàn toàn tokenizer từ đầu đến cuối mảnh từ và detokenizer từ tạp nham, và lưu nó như là một `saved_model` mà bạn có thể tải và sử dụng trong này [hướng dẫn dịch](https://tensorflow.org/text/tutorials/transformer)."
      ],
      "metadata": {
        "id": "LS4GXEHwjzZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tổng quan\r\n",
        "\r\n",
        "Các gói `tensorflow_text` bao gồm triển khai TensorFlow của nhiều tokenizer chung. Điều này bao gồm ba loại tokenizers kiểu từ con:\r\n",
        "\r\n",
        "* `text.BertTokenizer` - Lớp BertTokenizer là một giao diện cấp cao hơn. Nó bao gồm các thuật toán tách token của Bert và một `WordPieceTokenizer`. Nó lấy **các câu** như đầu vào và trả về các **token-ID**.\r\n",
        "\r\n",
        "* `text.WordpieceTokenizer` - Lớp `WordPieceTokenizer` là một giao diện cấp thấp hơn. Nó chỉ thực hiện [thuật toán WordPiece](https://www.tensorflow.org/text/guide/subwords_tokenizer#applying_wordpiece). Bạn phải chuẩn hóa và tách văn bản thành các từ trước khi gọi nó. Nó lấy **các từ** như đầu vào và trả về các **token-ID**.\r\n",
        "\r\n",
        "* `text.SentencepieceTokenizer` - `SentencepieceTokenizer` yêu cầu một thiết lập phức tạp hơn. Bộ khởi tạo của nó yêu cầu một mô hình mảnh câu tiền đào tạo. Xem [kho google/sentencepiece](https://github.com/google/sentencepiece#train-sentencepiece-model) để được hướng dẫn làm thế nào xây dựng một trong những mô hình này. Nó có thể chấp nhận **các câu** như đầu vào khi token hoá.\r\n",
        "\r\n",
        "Hướng dẫn này xây dựng một từ vựng Mảnh từ theo cách từ trên xuống, bắt đầu từ các từ hiện có. Quá trình này không hoạt động ở tiếng Nhật, tiếng Trung hoặc tiếng Hàn vì những ngôn ngữ này không có các đơn vị nhiều ký tự rõ ràng. Để token hoá các ngôn ngữ đó cân nhắc sử dụng `text.SentencepieceTokenizer`, `text.UnicodeCharTokenizer` hoặc [cách tiếp cận này](https://tfhub.dev/google/zh_segmentation/1)."
      ],
      "metadata": {
        "id": "PxOYBzVqE7mX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cài đặt\n",
        "\n"
      ],
      "metadata": {
        "id": "8e--LsVQqOI7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install -q -U tensorflow-text"
      ],
      "outputs": [],
      "metadata": {
        "id": "SrcCCuMhqQ9E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install -q tensorflow_datasets"
      ],
      "outputs": [],
      "metadata": {
        "id": "zb0G1l_oqRnr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import collections\r\n",
        "import os\r\n",
        "import pathlib\r\n",
        "import re\r\n",
        "import string\r\n",
        "import sys\r\n",
        "import tempfile\r\n",
        "import time\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "import tensorflow_text as text\r\n",
        "import tensorflow as tf"
      ],
      "outputs": [],
      "metadata": {
        "id": "m7I3jsrbqS5x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tf.get_logger().setLevel('ERROR')\n",
        "pwd = pathlib.Path.cwd()"
      ],
      "outputs": [],
      "metadata": {
        "id": "caetoEsgqU9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tải về tập dữ liệu\n",
        "\n",
        "Lấy tập dữ liệu dịch tiếng Bồ Đào Nha/Anh từ [tfds](https://tensorflow.org/datasets):"
      ],
      "metadata": {
        "id": "Zly47bRHqXiK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
        "                               as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "outputs": [],
      "metadata": {
        "id": "7b3xv1_-qjCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tập dữ liệu này tạo ra các cặp câu tiếng Bồ Đào Nha/Anh:"
      ],
      "metadata": {
        "id": "seQT2q2sqlS3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for pt, en in train_examples.take(1):\r\n",
        "  print(\"Portuguese: \", pt.numpy().decode('utf-8'))\r\n",
        "  print(\"English:   \", en.numpy().decode('utf-8'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "naQZe5HbqyDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lưu ý một số điều về các câu ví dụ ở trên:\n",
        "\n",
        "* Chúng là chữ thường.\n",
        "* Có khoảng trắng xung quanh dấu câu.\n",
        "* Không rõ liệu chuẩn hóa unicode có đang được sử dụng hay không."
      ],
      "metadata": {
        "id": "T0DaiHgPq2kE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_en = train_examples.map(lambda pt, en: en)\r\n",
        "train_pt = train_examples.map(lambda pt, en: pt)"
      ],
      "outputs": [],
      "metadata": {
        "id": "huSfCz59rAHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tạo từ vựng\r\n",
        "\r\n",
        "Phần này tạo ra một từ vựng mảnh từ từ một tập dữ liệu. Nếu bạn đã có một tập tin từ vựng và chỉ muốn xem làm thế nào để xây dựng một `text.BertTokenizer` hoặc tolenizer `text.Wordpiece` với nó thì bạn có thể bỏ qua thẳng đến phần [xây dựng các tokenizer](https://www.tensorflow.org/text/guide/subwords_tokenizer#build_the_tokenizer).\r\n",
        "\r\n",
        "Lưu ý: Mã sinh từ vựng được sử dụng trong hướng dẫn này được tối ưu hóa cho **đơn giản**. Nếu bạn cần một giải pháp mở rộng hơn xem xét sử dụng triển khai Apache Beam sẵn có trong [tools/wordpiece_vocab/generate_vocab.py](https://github.com/tensorflow/text/blob/master/tensorflow_text/tools/wordpiece_vocab/generate_vocab.py)\r\n",
        "\r\n",
        "\r\n",
        "Mã sinh từ vựng được bao gồm trong gói pip `tensorflow_text`. Nó không được nhập mặc định nên bạn cần phải nhập thủ công:"
      ],
      "metadata": {
        "id": "l7jW_PuprCuu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
      ],
      "outputs": [],
      "metadata": {
        "id": "_LqwHy2jrCY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm `bert_vocab.bert_vocab_from_dataset` sẽ tạo ra các từ vựng.\r\n",
        "\r\n",
        "Có nhiều đối số bạn có thể đặt để điều chỉnh hành vi của nó. Đối với hướng dẫn này, bạn sẽ chủ yếu sử dụng các giá trị mặc định. Nếu bạn muốn tìm hiểu thêm về các tùy chọn, đầu tiên đọc [thuật toán](https://www.tensorflow.org/text/guide/subwords_tokenizer#optional_the_algorithm), và sau đó có nhìn vào [mã này](https://github.com/tensorflow/text/blob/master/tensorflow_text/tools/wordpiece_vocab/bert_vocab_from_dataset.py).\r\n",
        "\r\n",
        "Quá trình này mất khoảng 2 phút."
      ],
      "metadata": {
        "id": "TisYjtzur5P3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "bert_tokenizer_params=dict(lower_case=True)\r\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\r\n",
        "\r\n",
        "bert_vocab_args = dict(\r\n",
        "    # The target vocabulary size\r\n",
        "    vocab_size = 8000,\r\n",
        "    # Reserved tokens that must be included in the vocabulary\r\n",
        "    reserved_tokens=reserved_tokens,\r\n",
        "    # Arguments for `text.BertTokenizer`\r\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\r\n",
        "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\r\n",
        "    learn_params={},\r\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "16kSnNjKsRSu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "pt_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_pt.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "UuL3o6zKsTfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đây là các vết cắt của kết quả từ vựng."
      ],
      "metadata": {
        "id": "D0TMfvvJsU92"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(pt_vocab[:10])\n",
        "print(pt_vocab[100:110])\n",
        "print(pt_vocab[1000:1010])\n",
        "print(pt_vocab[-10:])"
      ],
      "outputs": [],
      "metadata": {
        "id": "NAQ2O4bFsh1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Viết tập tin từ vựng:"
      ],
      "metadata": {
        "id": "vePXlMtJskR4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)"
      ],
      "outputs": [],
      "metadata": {
        "id": "-9rxQJ4asme7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "write_vocab_file('pt_vocab.txt', pt_vocab)"
      ],
      "outputs": [],
      "metadata": {
        "id": "QomCN--Wsp7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sử dụng hàm đó để tạo từ vựng từ dữ liệu tiếng Anh:"
      ],
      "metadata": {
        "id": "6wvu8Satsrqf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_en.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "E5P05WvUsu3C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(en_vocab[:10])\r\n",
        "print(en_vocab[100:110])\r\n",
        "print(en_vocab[1000:1010])\r\n",
        "print(en_vocab[-10:])"
      ],
      "outputs": [],
      "metadata": {
        "id": "-_3WNyetswmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đây là hai tập tin từ vựng:"
      ],
      "metadata": {
        "id": "eg4J0qf2sx1m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "write_vocab_file('en_vocab.txt', en_vocab)"
      ],
      "outputs": [],
      "metadata": {
        "id": "14HvaAkfs1GD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!ls *.txt"
      ],
      "outputs": [],
      "metadata": {
        "id": "VhSsa4Aas2RF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Xây dựng tokenizer\r\n",
        "\r\n",
        "`text.BertTokenizer` có thể được khởi tạo bằng cách truyền đường dẫn của tập tin từ vựng làm đối số đầu tiên (xem phần [tf.lookup](https://www.tensorflow.org/text/guide/subwords_tokenizer#optional_tflookup) để biết các tùy chọn khác):"
      ],
      "metadata": {
        "id": "NFSRsEw3s4I7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pt_tokenizer = text.BertTokenizer('pt_vocab.txt', **bert_tokenizer_params)\r\n",
        "en_tokenizer = text.BertTokenizer('en_vocab.txt', **bert_tokenizer_params)"
      ],
      "outputs": [],
      "metadata": {
        "id": "1IXwU2QttIOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bây giờ bạn có thể sử dụng nó để mã hóa một số văn bản. Lấy một lô 3 ví dụ từ dữ liệu tiếng Anh:"
      ],
      "metadata": {
        "id": "T2CU5eRatKhX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\r\n",
        "  for ex in en_examples:\r\n",
        "    print(ex.numpy())"
      ],
      "outputs": [],
      "metadata": {
        "id": "q4mGFL2CtNfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chạy nó thông qua phương thức `BertTokenizer.tokenize`. Ban đầu, nó trả về một `tf.RaggedTensor` với các trục `(batch, word, word-piece)`:"
      ],
      "metadata": {
        "id": "G8Gc0ZgytP_a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Tokenize the examples -> (batch, word, word-piece)\n",
        "token_batch = en_tokenizer.tokenize(en_examples)\n",
        "# Merge the word and word-piece axes -> (batch, tokens)\n",
        "token_batch = token_batch.merge_dims(-2,-1)\n",
        "\n",
        "for ex in token_batch.to_list():\n",
        "  print(ex)"
      ],
      "outputs": [],
      "metadata": {
        "id": "QnA4CtxUtZkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nếu bạn thay thế các ID token bằng các biểu diễn văn bản của chúng (sử dụng `tf.gather`), bạn có thể thấy rằng trong ví dụ đầu tiên, các từ \"`searchability`\" và \"`serendipity`\" đã được phân tách thành \"`search ##ability`\" và \"`s ##ere ##nd ##ip ##ity`\":"
      ],
      "metadata": {
        "id": "hmrhF3r1tbLx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Lookup each token id in the vocabulary.\n",
        "txt_tokens = tf.gather(en_vocab, token_batch)\n",
        "# Join with spaces.\n",
        "tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "OtyHqMLytwtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Để tập hợp lại các từ từ các token được trích xuất, sử dụng phương thức `BertTokenizer.detokenize`:\n",
        "\n",
        "> Lưu ý: `BertTokenizer.tokenize`/`BertTokenizer.detokenize` không khứ hồi losslessly. Kết quả của `detokenize` sẽ không, nói chung, có cùng một nội dung hoặc các bù đắp như đầu vào để `tokenize`. Điều này là do bước \"token hoá cơ bản\", để tách chuỗi thành các từ trước khi áp dụng `WordpieceTokenizer`, bao gồm các bước không thể đảo ngược như chữ thường và tách theo dấu câu. `WordpieceTokenizer` ở mặt khác **có thể đảo ngược**.\n"
      ],
      "metadata": {
        "id": "SZBCzayJtzZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tùy chỉnh và xuất\n",
        "\n",
        "Hướng dẫn này được xây dựng tokenizer văn bản và detokenizer sử dụng bởi các hướng dẫn [Transformer](https://tensorflow.org/text/tutorials/transformer). Phần này thêm các phương thức và các bước xử lý để đơn giản hóa hướng dẫn, và xuất các tokenizer sử dụng `tf.saved_model` để họ có thể được nhập bởi các hướng dẫn khác."
      ],
      "metadata": {
        "id": "D8u0ev-Lu3jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tuỳ chỉnh token hoá\n",
        "\n",
        "Các hướng dẫn dưới đây mong đợi văn bản đã token hoá bao gồm các token `[START]` và `[END]`.\n",
        "\n",
        "Các `reserved_tokens` giữ lại không gian ở phần đầu của từ vựng, vì vậy `[START]` và `[END]` có cùng các chỉ số ho cả hai ngôn ngữ:"
      ],
      "metadata": {
        "id": "UDZtHONrvQFS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
        "\n",
        "def add_start_end(ragged):\n",
        "  count = ragged.bounding_shape()[0]\n",
        "  starts = tf.fill([count,1], START)\n",
        "  ends = tf.fill([count,1], END)\n",
        "  return tf.concat([starts, ragged, ends], axis=1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "SqU_d8Pov5Q4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "words = en_tokenizer.detokenize(add_start_end(token_batch))\n",
        "tf.strings.reduce_join(words, separator=' ', axis=-1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "pBNOTaQBv6F1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tuỳ chỉnh detoken hoá\n",
        "\n",
        "Trước khi xuất các `tokenizer`, có một số thứ bạn có thể dọn dẹp cho các hướng dẫn phía dưới:\n",
        "\n",
        "1. Họ muốn tạo đầu ra văn bản sạch, vì vậy thả tokens để dành như `[START]`, `[END]` và `[PAD]`.\n",
        "\n",
        "2. Họ đang quan tâm trong các chuỗi hoàn chỉnh, vì vậy áp dụng một chuỗi nối dọc theo trục `words` của kết quả."
      ],
      "metadata": {
        "id": "wJn-yVpZv77f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def cleanup_text(reserved_tokens, token_txt):\n",
        "  # Drop the reserved tokens, except for \"[UNK]\".\n",
        "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "  bad_token_re = \"|\".join(bad_tokens)\n",
        "    \n",
        "  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
        "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "\n",
        "  # Join them into strings.\n",
        "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "\n",
        "  return result"
      ],
      "outputs": [],
      "metadata": {
        "id": "DwCVzfkFweQp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "en_examples.numpy()"
      ],
      "outputs": [],
      "metadata": {
        "id": "FZyOPBQQwfkd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "token_batch = en_tokenizer.tokenize(en_examples).merge_dims(-2,-1)\n",
        "words = en_tokenizer.detokenize(token_batch)\n",
        "words"
      ],
      "outputs": [],
      "metadata": {
        "id": "JO9qsa1Fwg8J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "cleanup_text(reserved_tokens, words).numpy()"
      ],
      "outputs": [],
      "metadata": {
        "id": "ko20Z9g-wib0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xuất\n",
        "\n",
        "Khối mã sau xây dựng một lớp `CustomTokenizer` để chứa các thể hiện `text.BertTokenizer`, logic tùy chỉnh, và lớp bọc `@tf.function` cần thiết cho xuất."
      ],
      "metadata": {
        "id": "Fhubd2cOwky1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class CustomTokenizer(tf.Module):\n",
        "  def __init__(self, reserved_tokens, vocab_path):\n",
        "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
        "    self._reserved_tokens = reserved_tokens\n",
        "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
        "\n",
        "    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
        "    self.vocab = tf.Variable(vocab)\n",
        "\n",
        "    ## Create the signatures for export:   \n",
        "\n",
        "    # Include a tokenize signature for a batch of strings. \n",
        "    self.tokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
        "    \n",
        "    # Include `detokenize` and `lookup` signatures for:\n",
        "    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
        "    #   * `RaggedTensors` with shape [batch, tokens]\n",
        "    self.detokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.detokenize.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    self.lookup.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.lookup.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    # These `get_*` methods take no arguments\n",
        "    self.get_vocab_size.get_concrete_function()\n",
        "    self.get_vocab_path.get_concrete_function()\n",
        "    self.get_reserved_tokens.get_concrete_function()\n",
        "    \n",
        "  @tf.function\n",
        "  def tokenize(self, strings):\n",
        "    enc = self.tokenizer.tokenize(strings)\n",
        "    # Merge the `word` and `word-piece` axes.\n",
        "    enc = enc.merge_dims(-2,-1)\n",
        "    enc = add_start_end(enc)\n",
        "    return enc\n",
        "\n",
        "  @tf.function\n",
        "  def detokenize(self, tokenized):\n",
        "    words = self.tokenizer.detokenize(tokenized)\n",
        "    return cleanup_text(self._reserved_tokens, words)\n",
        "\n",
        "  @tf.function\n",
        "  def lookup(self, token_ids):\n",
        "    return tf.gather(self.vocab, token_ids)\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_size(self):\n",
        "    return tf.shape(self.vocab)[0]\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_path(self):\n",
        "    return self._vocab_path\n",
        "\n",
        "  @tf.function\n",
        "  def get_reserved_tokens(self):\n",
        "    return tf.constant(self._reserved_tokens)"
      ],
      "outputs": [],
      "metadata": {
        "id": "yIyKM5kCw999"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xây dựng một `CustomTokenizer` cho mỗi ngôn ngữ:"
      ],
      "metadata": {
        "id": "L7oBH5R2w_PU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tokenizers = tf.Module()\n",
        "tokenizers.pt = CustomTokenizer(reserved_tokens, 'pt_vocab.txt')\n",
        "tokenizers.en = CustomTokenizer(reserved_tokens, 'en_vocab.txt')"
      ],
      "outputs": [],
      "metadata": {
        "id": "On9nVyCOxBdL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xuất các tokenizer dưới dạng `save_model`:"
      ],
      "metadata": {
        "id": "wtklW4lfxE_N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
        "tf.saved_model.save(tokenizers, model_name)"
      ],
      "outputs": [],
      "metadata": {
        "id": "OkSSucA0xIEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tải lại `saved_model` và kiểm tra các phương thức:"
      ],
      "metadata": {
        "id": "vQv0QQwOxJTr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "reloaded_tokenizers = tf.saved_model.load(model_name)\n",
        "reloaded_tokenizers.en.get_vocab_size().numpy()"
      ],
      "outputs": [],
      "metadata": {
        "id": "t48M_dI-xOsR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tokens = reloaded_tokenizers.en.tokenize(['Hello TensorFlow!'])\n",
        "tokens.numpy()"
      ],
      "outputs": [],
      "metadata": {
        "id": "zVwu0EfXxPoI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "text_tokens = reloaded_tokenizers.en.lookup(tokens)\n",
        "text_tokens"
      ],
      "outputs": [],
      "metadata": {
        "id": "5zc9CogixQrC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "round_trip = reloaded_tokenizers.en.detokenize(tokens)\n",
        "\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "PVnkakVFxR8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lưu trữ nó cho [các hướng dẫn dịch](https://tensorflow.org/text/tutorials/transformer):"
      ],
      "metadata": {
        "id": "c8EtW-7cxTvh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!zip -r {model_name}.zip {model_name}"
      ],
      "outputs": [],
      "metadata": {
        "id": "ijBl6-UNxW-H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!du -h *.zip"
      ],
      "outputs": [],
      "metadata": {
        "id": "-rWwV0xqxYLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuỳ chọn: Thuật toán\n",
        "\n",
        "Điều đáng chú ý ở đây là có hai phiên bản của thuật toán WordPiece: Dưới-lên và trên-xuống. Trong cả hai trường hợp, mục tiêu là như nhau: \"*Với một kho ngữ liệu đào tạo và một số token D mong muốn, vấn đề tối ưu hóa là chọn các mảnh từ D sao cho kho ngữ liệu thu được là tối thiểu về số lượng mảnh từ chữ khi được phân đoạn theo mô hình mảnh từ đã chọn.*\"\n",
        "\n",
        "Bản gốc [thuật toán WordPiece từ dưới-lên](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf), dựa trên [mã hóa cặp byte](https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10) . Giống như BPE, Nó bắt đầu với bảng chữ cái và kết hợp lặp đi lặp lại các bigram thông thường để tạo thành các mảnh từ và từ.\n",
        "\n",
        "Bộ tạo từ vựng của TensorFlow Text theo sau việc thực hiện từ trên-xuống từ [Bert](https://arxiv.org/pdf/1810.04805.pdf). Bắt đầu với các từ và chia nhỏ chúng thành các thành phần nhỏ hơn cho đến khi chúng đạt đến ngưỡng tần suất, hoặc không thể chia nhỏ hơn nữa. Phần tiếp theo mô tả chi tiết điều này. Đối với tiếng Nhật, tiếng Trung và tiếng Hàn, cách tiếp cận từ trên-xuống này không hoạt động vì không có đơn vị từ rõ ràng nào để bắt đầu. Đối với những bạn cần một [cách tiếp cận khác](https://tfhub.dev/google/zh_segmentation/1).\n",
        "\n",
        "(Phần sau dài quá nên lười... 😁)"
      ],
      "metadata": {
        "id": "6gHNy3xpxZta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing the vocabulary\n",
        "\n",
        "The top-down WordPiece generation algorithm takes in a set of (word, count) pairs and a threshold `T`, and returns a vocabulary `V`.\n",
        "\n",
        "The algorithm is iterative. It is run for `k` iterations, where typically `k = 4`, but only the first two are really important. The third and fourth (and beyond) are just identical to the second. Note that each step of the binary search runs the algorithm from scratch for `k` iterations.\n",
        "\n",
        "The iterations described below:"
      ],
      "metadata": {
        "id": "mp2QtW1gy6-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### First iteration\n",
        "\n",
        "1.  Iterate over every word and count pair in the input, denoted as `(w, c)`.\n",
        "2.  For each word `w`, generate every substring, denoted as `s`. E.g., for the\n",
        "    word `human`, we generate `{h, hu, hum, huma,\n",
        "    human, ##u, ##um, ##uma, ##uman, ##m, ##ma, ##man, #a, ##an, ##n}`.\n",
        "3.  Maintain a substring-to-count hash map, and increment the count of each `s`\n",
        "    by `c`. E.g., if we have `(human, 113)` and `(humas, 3)` in our input, the\n",
        "    count of `s = huma` will be `113+3=116`.\n",
        "4.  Once we've collected the counts of every substring, iterate over the `(s,\n",
        "    c)` pairs *starting with the longest `s` first*.\n",
        "5.  Keep any `s` that has a `c > T`. E.g., if `T = 100` and we have `(pers,\n",
        "    231); (dogs, 259); (##rint; 76)`, then we would keep `pers` and `dogs`.\n",
        "6.  When an `s` is kept, subtract off its count from all of its prefixes. This\n",
        "    is the reason for sorting all of the `s` by length in step 4. This is a\n",
        "    critical part of the algorithm, because otherwise words would be double\n",
        "    counted. For example, let's say that we've kept `human` and we get to\n",
        "    `(huma, 116)`. We know that `113` of those `116` came from `human`, and `3`\n",
        "    came from `humas`. However, now that `human` is in our vocabulary, we know\n",
        "    we will never segment `human` into `huma ##n`. So once `human` has been\n",
        "    kept, then `huma` only has an *effective* count of `3`.\n",
        "\n",
        "This algorithm will generate a set of word pieces `s` (many of which will be\n",
        "whole words `w`), which we *could* use as our WordPiece vocabulary.\n",
        "\n",
        "However, there is a problem: This algorithm will severely overgenerate word\n",
        "pieces. The reason is that we only subtract off counts of prefix tokens.\n",
        "Therefore, if we keep the word `human`, we will subtract off the count for `h,\n",
        "hu, hu, huma`, but not for `##u, ##um, ##uma, ##uman` and so on. So we might\n",
        "generate both `human` and `##uman` as word pieces, even though `##uman` will\n",
        "never be applied.\n",
        "\n",
        "So why not subtract off the counts for every *substring*, not just every\n",
        "*prefix*? Because then we could end up subtracting off the counts multiple\n",
        "times. Let's say that we're processing `s` of length 5 and we keep both\n",
        "`(##denia, 129)` and `(##eniab, 137)`, where `65` of those counts came from the\n",
        "word `undeniable`. If we subtract off from *every* substring, we would subtract\n",
        "`65` from the substring `##enia` twice, even though we should only subtract\n",
        "once. However, if we only subtract off from prefixes, it will correctly only be\n",
        "subtracted once."
      ],
      "metadata": {
        "id": "IXFtLt0by8_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying WordPiece\n",
        "\n",
        "<a id=\"applying_wordpiece\"></a>\n",
        "\n",
        "Once a WordPiece vocabulary has been generated, we need to be able to apply it\n",
        "to new data. The algorithm is a simple greedy longest-match-first application.\n",
        "\n",
        "For example, consider segmenting the word `undeniable`.\n",
        "\n",
        "We first lookup `undeniable` in our WordPiece dictionary, and if it's present,\n",
        "we're done. If not, we decrement the end point by one character, and repeat,\n",
        "e.g., `undeniabl`.\n",
        "\n",
        "Eventually, we will either find a subtoken in our vocabulary, or get down to a\n",
        "single character subtoken. (In general, we assume that every character is in our\n",
        "vocabulary, although this might not be the case for rare Unicode characters. If\n",
        "we encounter a rare Unicode character that's not in the vocabulary we simply map\n",
        "the entire word to `<unk>`).\n",
        "\n",
        "In this case, we find `un` in our vocabulary. So that's our first word piece.\n",
        "Then we jump to the end of `un` and repeat the processing, e.g., try to find\n",
        "`##deniable`, then `##deniabl`, etc. This is repeated until we've segmented the\n",
        "entire word."
      ],
      "metadata": {
        "id": "byOuWJrx0gHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: tf.lookup\n",
        "\n",
        "<a id=\"tf.lookup\"></a>"
      ],
      "metadata": {
        "id": "aHDTYkDi0jvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you need access to, or more control over the vocabulary it's worth noting that you can build the lookup table yourself and pass that to `BertTokenizer`.\n",
        "\n",
        "When you pass a string, `BertTokenizer` does the following:"
      ],
      "metadata": {
        "id": "AIOEoXpm0kk-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pt_lookup = tf.lookup.StaticVocabularyTable(\n",
        "    num_oov_buckets=1,\n",
        "    initializer=tf.lookup.TextFileInitializer(\n",
        "        filename='pt_vocab.txt',\n",
        "        key_dtype=tf.string,\n",
        "        key_index = tf.lookup.TextFileIndex.WHOLE_LINE,\n",
        "        value_dtype = tf.int64,\n",
        "        value_index=tf.lookup.TextFileIndex.LINE_NUMBER)) \n",
        "pt_tokenizer = text.BertTokenizer(pt_lookup)"
      ],
      "outputs": [],
      "metadata": {
        "id": "05TLvUmD0m_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you have direct access to the lookup table used in the tokenizer."
      ],
      "metadata": {
        "id": "Ir-eeDMA0pSH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pt_lookup.lookup(tf.constant(['é', 'um', 'uma', 'para', 'não']))"
      ],
      "outputs": [],
      "metadata": {
        "id": "Y_HNkH_L0rBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You don't need to use a vocabulary file, `tf.lookup` has other initializer options. If you have the vocabulary in memory you can use `lookup.KeyValueTensorInitializer`:"
      ],
      "metadata": {
        "id": "_aERcCr60tdI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pt_lookup = tf.lookup.StaticVocabularyTable(\n",
        "    num_oov_buckets=1,\n",
        "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=pt_vocab,\n",
        "        values=tf.range(len(pt_vocab), dtype=tf.int64))) \n",
        "pt_tokenizer = text.BertTokenizer(pt_lookup)"
      ],
      "outputs": [],
      "metadata": {
        "id": "KjJtBYLp0uiM"
      }
    }
  ]
}