{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "bert_preprocessing_guide",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYWYmcQEtZL+FdSzBTSRg9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2021 The TensorFlow Authors.\r\n",
        "\r\n",
        "```\r\n",
        "@title Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\r\n",
        "You may obtain a copy of the License at\r\n",
        "https://www.apache.org/licenses/LICENSE-2.0\r\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\r\n",
        "```"
      ],
      "metadata": {
        "id": "rtr6X5EyYa41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tiền xử lý văn bản với BERT"
      ],
      "metadata": {
        "id": "EJ4w0gSTY1NH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tổng quan"
      ],
      "metadata": {
        "id": "tXtkIQNEgjNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiền xử lý văn bản là quá trình chuyển đổi từ đầu-đến-cuối của văn bản thô thành đầu vào số nguyên của mô hình. Các mô hình xử lý ngôn ngữ tự nhiên (NLP) thường đi kèm với hàng trăm (nếu không phải hàng nghìn) dòng mã Python để tiền xử lý văn bản. Tiền xử lý văn bản thường là một thách thức cho các mô hình bởi vì:\n",
        "* **Độ nghiêng phục vụ-huấn luyện**: Ngày càng khó khăn để đảm bảo rằng logic tiền xử lý của các đầu vào của mô hình là nhất quán ở tất cả các giai đoạn phát triển mô hình (chẳng hạn tiền huấn luyện, tinh chỉnh, đánh giá và suy luận). Sử dụng các siêu tham số khác nhau, token hoá, các thuật toán tiền xử lý chuỗi hoặc đơn giản là đóng gói các đầu vào mô hình không nhất quán ở các giai đoạn khác nhau có thể mang lại lỗi khó gỡ và các tác động không tốt cho mô hình.\n",
        "* **Hiệu quả và linh hoạt**: Trong khi tiền xử lý có thể làm ở ngoại tuyến (chẳng hạn bằng cách viết các đầu ra đã được xử lý vào tập tin lưu trên đĩa và sau đó xem xét lại các dữ liệu đã được tiền xử lý trong đường tin đầu vào), phương pháp này phát sinh thêm chi phí đọc-ghi dữ liệu. Tiền xử lý ngoại tuyến thường bất tiện nếu có các quyết định tiền xử lý cần xảy ra động. Thử nghiệm với một tuỳ chọn khác nhau sẽ yêu cầu tạo lại tập dữ liệu một lần nữa.\n",
        "* **Giao diện mô hình phức tạp**: Các mô hình văn bản dễ hiểu hơn nhiều khi các đầu vào của nó là các văn bản thuần. Nó cũng gây khó hiểu một mô hình khi đầu vào yêu cầu các bước mã hoá gián tiếp bổ sung. Giảm độ phức tạp của quá trình tiền xử lý được đánh giá cao cho quá trình sửa lỗi mô hình, phục vụ và đánh giá.\n",
        "\n",
        "Thêm vào đó, giao diện mô hình đơn giản hơn cũng tạo thêm sự thuận tiện để thử mô hình (chẳng hạn dùng để suy luận hay huấn luyện) trên các tập dữ liệu khác nhau, chưa được khám phá."
      ],
      "metadata": {
        "id": "iP1Au6RCZdPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiền xử lý dữ liệu với TF.Text"
      ],
      "metadata": {
        "id": "yz1M6ABqggB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sử dụng các API tiền xử lý văn bản của TF.Text, chúng ta có thể xây dựng một hàm tiền xử lý mà có thể chuyển đổi một tập dữ liệu văn bản của người dùng thành các đầu vào số nguyên của mô hình. Người dùng có thể đóng gói trực tiếp tiền xử lý như là một phần của mô hình để giảm bớt các vấn đề đã đề cập ở trên.\n",
        "\n",
        "Hướng dẫn này sẽ trình bày cách sử dụng các hoạt động tiền xử lý TF.Text để chuyển đổi dữ liệu văn bản thành các đầu vào cho mô hình BERT và các đầu vào cho nhiệm vụ tiền huấn luyện mặt nạ ngôn ngữ được mô tả trong \"*Masked LM and Masking Procedure*\" của [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf). Quá trình liên quan đến token hoá văn bản thành các đơn vị từ con, loại bỏ nội dung để vừa khít kích thước và trích xuất nhãn cho nhiệm vụ mô hình hoá mặt nạ ngôn ngữ."
      ],
      "metadata": {
        "id": "eLAvaC_8g20w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Văn bản\n",
        "\n",
        "Nhập các gói và thư viện cần thiết:"
      ],
      "metadata": {
        "id": "mpxwcm62iQhN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install -q -U tensorflow-text"
      ],
      "outputs": [],
      "metadata": {
        "id": "z3lVohotiX-U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import tensorflow as tf\r\n",
        "import tensorflow_text as text\r\n",
        "import functools"
      ],
      "outputs": [],
      "metadata": {
        "id": "eencWjK_ibxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dữ liệu của chúng ta chứa hai đặc trưng văn bản và chúng ta có thể tạo một ví dụ `tf.data.Dataset`. Mục tiêu của chúng ta là tạo một hàm mà chúng ta có thể cung cấp `Dataset.map()` vào huấn luyện mô hình."
      ],
      "metadata": {
        "id": "ABMazk_pidk1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "examples = {\r\n",
        "    \"text_a\": [\r\n",
        "      b\"Sponge bob Squarepants is an Avenger\",\r\n",
        "      b\"Marvel Avengers\"\r\n",
        "    ],\r\n",
        "    \"text_b\": [\r\n",
        "     b\"Barack Obama is the President.\",\r\n",
        "     b\"President is the highest office\"\r\n",
        "  ],\r\n",
        "}\r\n",
        "\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices(examples)\r\n",
        "next(iter(dataset))"
      ],
      "outputs": [],
      "metadata": {
        "id": "4vJPcmMli0rW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Toeken hoá\n",
        "\n",
        "Bước đầu tiên là chạy tiền xử lý chuỗi bất kỳ và token tập dữ liệu. Điều này có thể thực hiện bởi `text.BertTokenizer`, nó là một hàm `text.Splitter` token các câu thành các từ con hoặc các mảnh từ cho [mô hình BERT](https://github.com/google-research/bert) với một từ vựng được tạo ra từ [thuật toán Wordpiece](https://www.tensorflow.org/text/guide/subwords_tokenizer#optional_the_algorithm).\n",
        "\n",
        "Từ vựng có thể là các điểm kiểm tra BERT được tạo trước đó, hoặc bạn có thể tạo một từ mới trên dữ liệu của bạn. Đối với các mục đích của ví dụ này, hãy tạo một từ vựng đồ chơi."
      ],
      "metadata": {
        "id": "PHaj-qRDi6JN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "_VOCAB = [\r\n",
        "    # Special tokens\r\n",
        "    b\"[UNK]\", b\"[MASK]\", b\"[RANDOM]\", b\"[CLS]\", b\"[SEP]\",\r\n",
        "    # Suffixes\r\n",
        "    b\"##ack\", b\"##ama\", b\"##ger\", b\"##gers\", b\"##onge\", b\"##pants\",  b\"##uare\",\r\n",
        "    b\"##vel\", b\"##ven\", b\"an\", b\"A\", b\"Bar\", b\"Hates\", b\"Mar\", b\"Ob\",\r\n",
        "    b\"Patrick\", b\"President\", b\"Sp\", b\"Sq\", b\"bob\", b\"box\", b\"has\", b\"highest\",\r\n",
        "    b\"is\", b\"office\", b\"the\",\r\n",
        "]\r\n",
        "\r\n",
        "_START_TOKEN = _VOCAB.index(b\"[CLS]\")\r\n",
        "_END_TOKEN = _VOCAB.index(b\"[SEP]\")\r\n",
        "_MASK_TOKEN = _VOCAB.index(b\"[MASK]\")\r\n",
        "_RANDOM_TOKEN = _VOCAB.index(b\"[RANDOM]\")\r\n",
        "_UNK_TOKEN = _VOCAB.index(b\"[UNK]\")\r\n",
        "_MAX_SEQ_LEN = 8\r\n",
        "_MAX_PREDICTIONS_PER_BATCH = 5\r\n",
        " \r\n",
        "_VOCAB_SIZE = len(_VOCAB)\r\n",
        "\r\n",
        "lookup_table = tf.lookup.StaticVocabularyTable(\r\n",
        "    tf.lookup.KeyValueTensorInitializer(\r\n",
        "      keys=_VOCAB,\r\n",
        "      key_dtype=tf.string,\r\n",
        "      values=tf.range(\r\n",
        "          tf.size(_VOCAB, out_type=tf.int64), dtype=tf.int64),\r\n",
        "      value_dtype=tf.int64),\r\n",
        "      num_oov_buckets=1\r\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "guGyHwxF5IoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xây dựng một `text.BertTokenizer` sử dụng từ vựng ở trên và token đầu vào văn bản thành một `RaggedTensor`."
      ],
      "metadata": {
        "id": "Ypl7TpHF5MjQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "bert_tokenizer = text.BertTokenizer(lookup_table, token_out_type=tf.string)\r\n",
        "bert_tokenizer.tokenize(examples[\"text_a\"])"
      ],
      "outputs": [],
      "metadata": {
        "id": "9zQ_8tE95cx4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "bert_tokenizer.tokenize(examples[\"text_b\"])"
      ],
      "outputs": [],
      "metadata": {
        "id": "Q1z_CyZd5eX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đầu ra văn bản từ `text.BertTokenizer` cho phép chúng ta xem cách văn bản được token hoá, nhưng mô hình yêu câu các ID số nguyên. Chúng ta có thể đặt tham số `token_out_type` thành `tf.int64` để lấy các ID nguyên (là các chỉ số trong từ vựng)."
      ],
      "metadata": {
        "id": "RWFNMnqt5ioH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "bert_tokenizer = text.BertTokenizer(lookup_table, token_out_type=tf.int64)\r\n",
        "segment_a = bert_tokenizer.tokenize(examples[\"text_a\"])\r\n",
        "segment_a"
      ],
      "outputs": [],
      "metadata": {
        "id": "sGp0ahQW5_T5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "segment_b = bert_tokenizer.tokenize(examples[\"text_b\"])\r\n",
        "segment_b"
      ],
      "outputs": [],
      "metadata": {
        "id": "lecu2OnZ6Au9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`text.BertTokenizer` trả về một RaggedTensor với hình dạng `[batch, num_tokens, num_wordpieces]`. Bởi vì chúng ta không cần chiều `num_tokens` bổ sung cho trường hợp sử dụng hiện tại, chúng ta có thể gộp hai chiều cuối cùng để có được một `RaggedTensor` với hình dạng `[batch, num_wordpieces]`:"
      ],
      "metadata": {
        "id": "id2AupCm6JbR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "segment_a = segment_a.merge_dims(-2, -1)\r\n",
        "segment_a"
      ],
      "outputs": [],
      "metadata": {
        "id": "Q8xynjbp6ttt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "segment_b = segment_b.merge_dims(-2, -1)\r\n",
        "segment_b"
      ],
      "outputs": [],
      "metadata": {
        "id": "VChEaJgo6u7s"
      }
    }
  ]
}