{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classify_text_with_bert",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyObH8RTGbZ9vfnEmQ7aUkHm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAMXkoPhaIh6"
      },
      "source": [
        "Copyright 2020 The TensorFlow Hub Authors.\n",
        "\n",
        "```\n",
        "@title Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iH76QOvaP6q"
      },
      "source": [
        "# Phân loại văn bản với BERT\n",
        "\n",
        "Hướng dẫn này chứa mã hoàn chỉnh để tinh chỉnh BERT để thực hiện phân tích cảm xúc trên tập dữ liệu đánh giá phim IMDB ở dạng văn bản thuần túy. Ngoài việc đào tạo một mô hình, bạn sẽ học cách tiền xử lý văn bản thành một định dạng thích hợp.\n",
        "\n",
        "Trong sổ tay này, bạn sẽ:\n",
        "\n",
        "* Tải tập dữ liệu IMDB\n",
        "\n",
        "* Tải mô hình BERT từ TensorFlow Hub\n",
        "\n",
        "* Xây dựng mô hình của riêng bạn bằng cách kết hợp BERT với bộ phân loại\n",
        "\n",
        "* Đào tạo mô hình của riêng bạn, tinh chỉnh BERT như một phần của điều đó\n",
        "\n",
        "* Lưu mô hình của bạn và sử dụng nó để phân loại các câu\n",
        "\n",
        "Nếu bạn mới làm việc với tập dữ liệu IMDB, vui lòng xem [Phân loại văn bản cơ bản](https://www.tensorflow.org/tutorials/keras/text_classification) để biết thêm chi tiết."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi0vDXMaavDU"
      },
      "source": [
        "## Về BERT\n",
        "\n",
        "BERT và các kiến ​​trúc mã hóa Transformer khác đã rất thành công trên nhiều nhiệm vụ khác nhau trong NLP (xử lý ngôn ngữ tự nhiên). Họ tính toán các biểu diễn không gian véctơ của ngôn ngữ tự nhiên phù hợp để sử dụng trong các mô hình học sâu. Họ BERT của các mô hình sử dụng kiến ​​trúc bộ mã hóa Transformer để xử lý từng token của văn bản đầu vào trong ngữ cảnh đầy đủ của tất cả các token trước và sau, do đó có tên: **Biểu diễn Thể hiện Mã hóa Hai chiều từ Transformer**.\n",
        "\n",
        "Các mô hình BERT thường được tiền huấn luyện trên một kho văn bản lớn, sau đó được tinh chỉnh cho các tác vụ cụ thể."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkaSIJwRcewt"
      },
      "source": [
        "## Cài đặt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vHBcXJtcgjn"
      },
      "source": [
        "# A dependency of the preprocessing for BERT inputs\n",
        "!pip install -q -U tensorflow-text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IMwrXxUcivF"
      },
      "source": [
        "Bạn sẽ sử dụng trình tối ưu hóa AdamW từ [tensorflow/models](https://github.com/tensorflow/models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2gqPD7hcp3l"
      },
      "source": [
        "!pip install -q tf-models-official"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx-jJqiycrVt"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLGr-oxsctdt"
      },
      "source": [
        "## Phân tích cảm xúc\n",
        "\n",
        "Sổ tay này huấn luyện một mô hình phân tích cảm xúc để phân loại các bài đánh giá phim là *tích cực* hay *tiêu cực*, dựa trên nội dung của bài đánh giá.\n",
        "\n",
        "Bạn sẽ sử dụng [Tập dữ liệu đánh giá phim lớn](https://ai.stanford.edu/~amaas/data/sentiment/) chứa văn bản của 50.000 bài đánh giá phim từ [Cơ sở dữ liệu phim trên Internet](https://www.imdb.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRCf9HwRdG3H"
      },
      "source": [
        "### Tải về tập dữ liệu IMDB\n",
        "\n",
        "Hãy tải xuống và giải nén tập dữ liệu, sau đó khám phá cấu trúc thư mục."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MImQpjDLdQV8"
      },
      "source": [
        "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "\n",
        "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
        "                                  untar=True, cache_dir='.',\n",
        "                                  cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "\n",
        "# remove unused folders to make it easier to load the data\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWWkhnStdRXQ"
      },
      "source": [
        "Tiếp theo, bạn sẽ sử dụng tiện ích `text_dataset_from_directory` để tạo một `tf.data.Dataset` có nhãn.\n",
        "\n",
        "Tập dữ liệu IMDB đã được chia thành tập huấn luyện và kiểm tra, nhưng nó thiếu tập xác thực. Hãy tạo một tập xác thực bằng cách tách 80:20 của dữ liệu huấn luyện bằng cách sử dụng đối số `validation_split` bên dưới.\n",
        "\n",
        "Lưu ý: Khi sử dụng các đối số `validation_split` và `subset`, hãy đảm bảo chỉ định một seed ngẫu nhiên hoặc truyền `shuffle=False`, để tách tập xác thực và huấn luyện không có chồng chéo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L41jzwbvd2ki"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)\n",
        "\n",
        "class_names = raw_train_ds.class_names\n",
        "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)\n",
        "\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test',\n",
        "    batch_size=batch_size)\n",
        "\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__Dp0u2md3rr"
      },
      "source": [
        "Hãy cùng xem qua một vài đánh giá."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIcnNOX3d5_X"
      },
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(f'Review: {text_batch.numpy()[i]}')\n",
        "    label = label_batch.numpy()[i]\n",
        "    print(f'Label : {label} ({class_names[label]})')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2910o_Lyd8Cr"
      },
      "source": [
        "### Tải các mô hình từ TensorFlow Hub\n",
        "\n",
        "Tại đây, bạn có thể chọn mô hình BERT mà bạn sẽ tải từ TensorFlow Hub và tinh chỉnh. Có nhiều mô hình BERT có sẵn.\n",
        "\n",
        "* [BERT-Base](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3), [Uncased](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3) và [bảy mô hình khác](https://tfhub.dev/google/collections/bert/1) với trọng số được huấn luyện do các tác giả BERT ban đầu phát hành.\n",
        "\n",
        "* [Các BERT nhỏ](https://tfhub.dev/google/collections/bert/1) có cùng kiến ​​trúc chung nhưng ít hơn và/hoặc các khối Transformer nhỏ hơn, cho phép bạn khám phá sự đánh đổi giữa tốc độ, kích thước và chất lượng.\n",
        "\n",
        "* [ALBERT](https://tfhub.dev/google/collections/albert/1): bốn kích thước khác nhau của \"*A Lite BERT*\" giúp giảm kích thước mô hình (nhưng không phải thời gian tính toán) bằng cách chia sẻ các tham số giữa các lớp.\n",
        "\n",
        "* [Các chuyên gia BERT](https://tfhub.dev/google/collections/experts/bert/1): tám mô hình đều có kiến ​​trúc cơ sở BERT nhưng cung cấp sự lựa chọn giữa các miền tiền huấn luyện khác nhau, để điều chỉnh chặt chẽ hơn với nhiệm vụ mục tiêu.\n",
        "\n",
        "* [Electra](https://tfhub.dev/google/collections/electra/1) có kiến ​​trúc tương tự như BERT (ở ba kích thước khác nhau), nhưng được tiền huấn luyện như một người phân biệt trong một thiết lập giống như *Mạng đối nghịch tạo sinh (GAN)*.\n",
        "\n",
        "* BERT với Talking-Heads Attention và Gated GELU [[cơ bản](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1), [lớn](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1)] có hai cải tiến đối với lõi của kiến ​​trúc Transformer.\n",
        "\n",
        "Tài liệu mô hình trên TensorFlow Hub có nhiều chi tiết hơn và tham chiếu đến các tài liệu nghiên cứu. Theo các liên kết ở trên hoặc nhấp vào URL [tfhub.dev](http://tfhub.dev/) được in sau khi thực thi ô tiếp theo.\n",
        "\n",
        "Đề xuất là bắt đầu với BERT nhỏ (với ít tham số hơn) vì chúng nhanh hơn để tinh chỉnh. Nếu bạn thích một mô hình nhỏ nhưng có độ chính xác cao hơn, ALBERT có thể là lựa chọn tiếp theo của bạn. Nếu bạn muốn độ chính xác cao hơn nữa, hãy chọn một trong các kích thước BERT cổ điển hoặc các tinh chỉnh gần đây của chúng như Electra, Talking Heads hoặc một BERT Expert.\n",
        "\n",
        "Ngoài các mô hình có sẵn bên dưới, có [nhiều phiên bản](https://tfhub.dev/google/collections/transformer_encoders_text/1) của các mô hình lớn hơn và có thể mang lại độ chính xác tốt hơn, nhưng chúng quá lớn để có thể tinh chỉnh trên một GPU. Bạn sẽ có thể làm điều đó trên các nhiệm vụ [Giải quyết các nhiệm vụ GLUE bằng BERT trên một TPU colab](https://www.tensorflow.org/text/tutorials/bert_glue).\n",
        "\n",
        "Bạn sẽ thấy trong đoạn mã bên dưới rằng việc chuyển đổi URL tfhub.dev là đủ để thử bất kỳ mô hình nào trong số này, vì tất cả sự khác biệt giữa chúng được gói gọn trong SavedModels từ TF Hub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXRomrZQgHhq"
      },
      "source": [
        "#@title Choose a BERT model to fine-tune\n",
        "\n",
        "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzQVuiQTgL-d"
      },
      "source": [
        "### Mô hình tiền xử lý\n",
        "\n",
        "Các đầu vào văn bản cần được chuyển đổi thành token id số và được sắp xếp thành nhiều Tensors trước khi được nhập vào BERT. TensorFlow Hub cung cấp một mô hình tiền xử lý phù hợp cho từng mô hình BERT được thảo luận ở trên, mô hình này thực hiện chuyển đổi này bằng cách sử dụng các hoạt động TF từ thư viện `TF.text`. Không cần thiết phải chạy mã Python thuần túy bên ngoài mô hình TensorFlow của bạn để tiền xử lý văn bản.\n",
        "\n",
        "Mô hình tiền xử lý phải là mô hình được tham chiếu bởi tài liệu của mô hình BERT, bạn có thể đọc mô hình này tại URL được in ở trên. Đối với các mô hình BERT từ menu thả xuống ở trên, mô hình tiền xử lý được chọn tự động.\n",
        "\n",
        "Lưu ý: Bạn sẽ tải mô hình tiền xử lý vào một [hub.KerasLayer](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) để soạn mô hình đã tinh chỉnh của bạn. Đây là API ưa tiên để tải SavedModel kiểu TF2 từ TF Hub vào mô hình Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGaFY1g-g7_Z"
      },
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfQsUEYvg9yC"
      },
      "source": [
        "Hãy thử mô hình tiền xử lý trên một số văn bản và xem kết quả:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6uCU88zg_ea"
      },
      "source": [
        "text_test = ['this is such an amazing movie!']\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
        "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
        "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3yNQbEwhCWy"
      },
      "source": [
        "Như bạn có thể thấy, bây giờ bạn có 3 đầu ra từ tiền xử lý mà mô hình BERT sẽ sử dụng `(input_words_id, input_mask` và `input_type_ids)`.\n",
        "\n",
        "Một số điểm quan trọng khác:\n",
        "\n",
        "* Đầu vào được cắt ngắn thành 128 token. Số lượng token có thể được tùy chỉnh và bạn có thể xem thêm chi tiết về [Giải quyết các nhiệm vụ GLUE bằng BERT trên một TPU colab](https://www.tensorflow.org/text/tutorials/bert_glue).\n",
        "\n",
        "* Các `input_type_ids` chỉ có một giá trị (0) vì đây là một đầu vào câu đơn. Đối với đầu vào nhiều câu, nó sẽ có một số cho mỗi đầu vào.\n",
        "\n",
        "Vì bộ tiền xử lý văn bản này là một mô hình TensorFlow, nên nó có thể được đưa trực tiếp vào mô hình của bạn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhPiwbrVhff3"
      },
      "source": [
        "### Sử dụng mô hình BERT\n",
        "\n",
        "Trước khi đưa BERT vào mô hình của riêng bạn, chúng ta hãy xem xét kết quả đầu ra của nó. Bạn sẽ tải nó từ TF Hub và xem các giá trị trả về."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wWz1eY-hkLr"
      },
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XNW_56Ohl-O"
      },
      "source": [
        "bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
        "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
        "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
        "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
        "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtdmTvI5holL"
      },
      "source": [
        "Các mô hình BERT trả về một bản đồ với 3 khóa quan trọng: `pooled_output`, `sequence_output`, `encoder_outputs`:\n",
        "\n",
        "* `pooled_output` đại diện cho toàn bộ chuỗi đầu vào. Hình dạng là `[batch_size, H]`. Bạn có thể coi đây là một phần nhúng cho toàn bộ bài đánh giá phim.\n",
        "\n",
        "* `sequence_output` đại diện cho mỗi token đầu vào trong ngữ cảnh. Hình dạng là `[batch_size, seq_length, H]`. Bạn có thể coi đây là cách nhúng theo ngữ cảnh cho mỗi token trong bài đánh giá phim.\n",
        "\n",
        "* `encoder_outputs` là các kích hoạt trung gian của các khối `L` Transformer. `outputs[\"encoder_outputs\"][i]` là Tensor của hình dạng `[batch_size, seq_length, 1024]` với các đầu ra của khối Transformer `thứ i`, cho `0 <= i < L`. Giá trị cuối cùng của danh sách bằng `sequence_output`.\n",
        "\n",
        "Để tinh chỉnh, bạn sẽ sử dụng mảng `pooled_output`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZqal1a0iWmd"
      },
      "source": [
        "### Định nghĩa mô hình của bạn\n",
        "\n",
        "Bạn sẽ tạo một mô hình được tinh chỉnh rất đơn giản, với mô hình tiền xử lý, mô hình BERT đã chọn, một lớp Dense và một lớp Dropout.\n",
        "\n",
        "Lưu ý: để biết thêm thông tin về đầu vào và đầu ra của mô hình cơ sở, bạn có thể theo dõi URL của mô hình để làm tài liệu. Cụ thể ở đây, bạn không cần phải lo lắng về điều đó vì mô hình tiền xử lý sẽ đảm nhận việc đó cho bạn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk12ZuZ4imZR"
      },
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwWOuhnNioeJ"
      },
      "source": [
        "Hãy kiểm tra xem mô hình có chạy với đầu ra của mô hình tiền xử lý hay không."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY770nL9iq8M"
      },
      "source": [
        "classifier_model = build_classifier_model()\n",
        "bert_raw_result = classifier_model(tf.constant(text_test))\n",
        "print(tf.sigmoid(bert_raw_result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BswbY3HOitJt"
      },
      "source": [
        "Tất nhiên, đầu ra là vô nghĩa vì người mô hình chưa được huấn luyện.\n",
        "\n",
        "Chúng ta hãy nhìn vào cấu trúc của mô hình."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yTsJ3S-ixgN"
      },
      "source": [
        "tf.keras.utils.plot_model(classifier_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_c54r_Ki1FH"
      },
      "source": [
        "### Đào tạo người mẫu\n",
        "\n",
        "Bây giờ bạn có tất cả các phần để huấn luyện một mô hình, bao gồm mô-đun tiền xử lý, bộ mã hóa BERT, dữ liệu và bộ phân loại."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNgCjZogjBfD"
      },
      "source": [
        "#### Hàm mất mát\n",
        "\n",
        "Vì đây là một vấn đề phân loại nhị phân và mô hình xuất ra một xác suất (một lớp đơn vị), bạn sẽ sử dụng hàm mất mát `losses.BinaryCrossentropy`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJVItLpvjP-J"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg_qcWwMjQdW"
      },
      "source": [
        "#### Trình tối ưu hóa\n",
        "\n",
        "Để tinh chỉnh, hãy sử dụng cùng một trình tối ưu hóa mà BERT đã được huấn luyện ban đầu: \"*Khoảnh khắc thích ứng*\" (Adam). Trình tối ưu hóa này giảm thiểu sự mất mát dự đoán và thực hiện điều hòa bằng phân rã trọng số (không sử dụng moment), còn được gọi là [AdamW](https://arxiv.org/abs/1711.05101) .\n",
        "\n",
        "Đối với tốc độ học (`init_lr`), bạn sẽ sử dụng cùng một lịch trình như tiền huấn luyện BERT: phân rã tuyến tính của tốc độ học ban đầu theo lý thuyết, tiền tố với giai đoạn khởi động tuyến tính trong 10% bước huấn luyện đầu tiên (`num_warmup_steps`). Phù hợp với bài báo BERT, tốc độ học ban đầu nhỏ hơn để tinh chỉnh (tốt nhất là 5e-5, 3e-5, 2e-5)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVfuEfdEkGPp"
      },
      "source": [
        "epochs = 5\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wRgdHTzkHLk"
      },
      "source": [
        "#### Tải mô hình BERT và huấn luyện\n",
        "\n",
        "Sử dụng `classifier_model` mà bạn đã tạo trước đó, bạn có thể biên dịch mô hình với mất mát, chỉ số đo lường và trình tối ưu hóa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejqDU5_ikUrr"
      },
      "source": [
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggb9juQtkW1Z"
      },
      "source": [
        "Lưu ý: thời gian huấn luyện sẽ thay đổi tùy thuộc vào độ phức tạp của mô hình BERT bạn đã chọn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LYUH0SVkZz8"
      },
      "source": [
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model.fit(x=train_ds,\n",
        "                               validation_data=val_ds,\n",
        "                               epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MACEaNxNkcTQ"
      },
      "source": [
        "#### Đánh giá mô hình\n",
        "\n",
        "Hãy xem mô hình hoạt động như thế nào. Hai giá trị sẽ được trả về. Mất mát (một số đại diện cho lỗi, các giá trị càng thấp thì càng tốt) và độ chính xác."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEd_KmN-kllD"
      },
      "source": [
        "loss, accuracy = classifier_model.evaluate(test_ds)\n",
        "\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TySXk-R0kn3t"
      },
      "source": [
        "#### Vẽ biểu đồ độ chính xác và mất mát theo thời gian\n",
        "\n",
        "Dựa trên đối tượng `History` trả về bởi `model.fit()`. Bạn có thể vẽ biểu đồ của quá trình huẩn luyện và mất mát xác thực để so sánh, cũng như độ chính xác của quá trình huấn luyện và xác thực:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esAn5rxRk0c1"
      },
      "source": [
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "\n",
        "acc = history_dict['binary_accuracy']\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1G5xnbUk2fk"
      },
      "source": [
        "Trong biểu đồ này, các đường màu đỏ thể hiện sự mất mát trong quá trình huấn luyện và độ chính xác, còn các đường màu xanh là sự mất mát xác thực và độ chính xác."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTqQ6vZPk8Qa"
      },
      "source": [
        "### Xuất dùng để suy luận\n",
        "\n",
        "Bây giờ bạn chỉ cần lưu mô hình đã tinh chỉnh của mình để sử dụng sau này."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qDzU-DZlBSF"
      },
      "source": [
        "dataset_name = 'imdb'\n",
        "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
        "\n",
        "classifier_model.save(saved_model_path, include_optimizer=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjO0hTJvlDSB"
      },
      "source": [
        "Hãy tải lại mô hình, để bạn có thể thử song song với mô hình vẫn còn trong bộ nhớ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRcQ78s4lFEy"
      },
      "source": [
        "reloaded_model = tf.saved_model.load(saved_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7CvB0jflIOk"
      },
      "source": [
        "Tại đây, bạn có thể kiểm tra mô hình của mình trên bất kỳ câu nào bạn muốn, chỉ cần thêm vào biến ví dụ bên dưới."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye68HrrblLUC"
      },
      "source": [
        "def print_my_examples(inputs, results):\n",
        "  result_for_printing = \\\n",
        "    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n",
        "                         for i in range(len(inputs))]\n",
        "  print(*result_for_printing, sep='\\n')\n",
        "  print()\n",
        "\n",
        "\n",
        "examples = [\n",
        "    'this is such an amazing movie!',  # this is the same sentence tried earlier\n",
        "    'The movie was great!',\n",
        "    'The movie was meh.',\n",
        "    'The movie was okish.',\n",
        "    'The movie was terrible...'\n",
        "]\n",
        "\n",
        "reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
        "original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n",
        "\n",
        "print('Results from the saved model:')\n",
        "print_my_examples(examples, reloaded_results)\n",
        "print('Results from the model in memory:')\n",
        "print_my_examples(examples, original_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4Pc1EwWlNsG"
      },
      "source": [
        "Nếu bạn muốn sử dụng mô hình của mình trên [TF Serving](https://www.tensorflow.org/tfx/guide/serving), hãy nhớ rằng nó sẽ gọi `SavedModel` của bạn thông qua một trong các chữ ký được đặt tên của nó. Trong Python, bạn có thể kiểm tra chúng như sau:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcsXtw51lVct"
      },
      "source": [
        "serving_results = reloaded_model \\\n",
        "            .signatures['serving_default'](tf.constant(examples))\n",
        "\n",
        "serving_results = tf.sigmoid(serving_results['classifier'])\n",
        "\n",
        "print_my_examples(examples, serving_results)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}